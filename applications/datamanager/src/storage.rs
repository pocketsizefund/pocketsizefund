use crate::data::{
    create_equity_bar_dataframe, create_equity_details_dataframe, create_portfolio_dataframe,
    create_predictions_dataframe, EquityBar, Portfolio, Prediction,
};
use crate::errors::Error;
use crate::state::State;
use aws_credential_types::provider::ProvideCredentials;
use aws_sdk_s3::primitives::ByteStream;
use chrono::{DateTime, Utc};
use duckdb::Connection;
use polars::prelude::*;
use serde::Deserialize;
use std::io::Cursor;
use tracing::{debug, info, warn};

const MIN_DATE_INT: i32 = 0;
const MAX_DATE_INT: i32 = 99999999;

pub async fn write_equity_bars_dataframe_to_s3(
    state: &State,
    dataframe: &DataFrame,
    timestamp: &DateTime<Utc>,
) -> Result<String, Error> {
    write_dataframe_to_s3(state, dataframe, timestamp, "bars".to_string()).await
}

pub async fn write_portfolio_dataframe_to_s3(
    state: &State,
    dataframe: &DataFrame,
    timestamp: &DateTime<Utc>,
) -> Result<String, Error> {
    write_dataframe_to_s3(state, dataframe, timestamp, "portfolios".to_string()).await
}

pub async fn write_predictions_dataframe_to_s3(
    state: &State,
    dataframe: &DataFrame,
    timestamp: &DateTime<Utc>,
) -> Result<String, Error> {
    write_dataframe_to_s3(state, dataframe, timestamp, "predictions".to_string()).await
}

async fn write_dataframe_to_s3(
    state: &State,
    dataframe: &DataFrame,
    timestamp: &DateTime<Utc>,
    dataframe_type: String,
) -> Result<String, Error> {
    info!("Uploading DataFrame to S3 as parquet");

    let year = timestamp.format("%Y");
    let month = timestamp.format("%m");
    let day = timestamp.format("%d");

    let key = format!(
        "equity/{}/daily/year={}/month={}/day={}/data.parquet",
        dataframe_type, year, month, day,
    );

    let mut buffer = Vec::new();
    {
        let cursor = Cursor::new(&mut buffer);
        let writer = ParquetWriter::new(cursor);
        match writer.finish(&mut dataframe.clone()) {
            Ok(_) => {
                info!(
                    "DataFrame successfully converted to parquet, size: {} bytes",
                    buffer.len()
                );
            }
            Err(err) => {
                return Err(Error::Other(format!("Failed to write parquet: {}", err)));
            }
        }
    }

    let body = ByteStream::from(buffer);

    match state
        .s3_client
        .put_object()
        .bucket(&state.bucket_name)
        .key(&key)
        .body(body)
        .content_type("application/octet-stream")
        .send()
        .await
    {
        Ok(_) => {
            info!(
                "Successfully uploaded parquet file to s3://{}/{}",
                state.bucket_name, key
            );
            Ok(key)
        }
        Err(err) => Err(Error::Other(format!("Failed to upload to S3: {}", err))),
    }
}

async fn create_duckdb_connection() -> Result<Connection, Error> {
    debug!("Opening in-memory DuckDB connection");
    let connection = Connection::open_in_memory()?;

    debug!("Installing and loading httpfs extension");
    connection.execute_batch("INSTALL httpfs; LOAD httpfs;")?;

    debug!("Loading AWS configuration for DuckDB S3 access");
    let config = aws_config::load_defaults(aws_config::BehaviorVersion::latest()).await;
    let provider = config.credentials_provider().ok_or_else(|| {
        warn!("No AWS credentials provider found");
        Error::Other("No AWS credentials provider found".into())
    })?;

    debug!("Fetching AWS credentials");
    let credentials = provider.provide_credentials().await?;

    let region = config
        .region()
        .map(|r| r.as_ref().to_string())
        .ok_or_else(|| Error::Other("AWS region must be configured".to_string()))?;

    let has_session_token = credentials.session_token().is_some();
    debug!(
        "AWS credentials loaded: region={}, has_session_token={}",
        region, has_session_token
    );

    let session_token = credentials.session_token().unwrap_or_default();
    let s3_config = format!(
        "
            SET s3_region='{}';
            SET s3_url_style='path';
            SET s3_access_key_id='{}';
            SET s3_secret_access_key='{}';
            SET s3_session_token='{}';
        ",
        region,
        credentials.access_key_id(),
        credentials.secret_access_key(),
        session_token
    );

    debug!("Configuring DuckDB S3 settings");
    connection.execute_batch(&s3_config)?;

    info!("DuckDB connection established with S3 access");
    Ok(connection)
}

pub async fn query_equity_bars_parquet_from_s3(
    state: &State,
    tickers: Option<Vec<String>>,
    start_timestamp: Option<DateTime<Utc>>,
    end_timestamp: Option<DateTime<Utc>>,
) -> Result<Vec<u8>, Error> {
    let connection = create_duckdb_connection().await?;

    let (start_timestamp, end_timestamp) = match (start_timestamp, end_timestamp) {
        (Some(start), Some(end)) => (start, end),
        _ => {
            let end_date = chrono::Utc::now();
            let start_date = end_date - chrono::Duration::days(7);
            info!(
                "No date range specified, using default: {} to {}",
                start_date, end_date
            );
            (start_date, end_date)
        }
    };

    info!(
        "Querying equity bars from {} to {}, bucket: {}",
        start_timestamp, end_timestamp, state.bucket_name
    );

    let s3_glob = format!(
        "s3://{}/equity/bars/daily/**/*.parquet",
        state.bucket_name
    );

    info!("Using S3 glob pattern: {}", s3_glob);

    let start_date_int = start_timestamp.format("%Y%m%d").to_string().parse::<i32>().unwrap_or(MIN_DATE_INT);
    let end_date_int = end_timestamp.format("%Y%m%d").to_string().parse::<i32>().unwrap_or(MAX_DATE_INT);

    debug!(
        "Date range filter: {} to {} (as integers)",
        start_date_int, end_date_int
    );

    let ticker_filter = match &tickers {
        Some(ticker_list) if !ticker_list.is_empty() => {
            debug!("Validating {} tickers for query filter", ticker_list.len());
            for ticker in ticker_list {
                if !ticker
                    .chars()
                    .all(|c| c.is_ascii_alphanumeric() || c == '.' || c == '-')
                {
                    warn!("Invalid ticker format rejected: {}", ticker);
                    return Err(Error::Other(format!("Invalid ticker format: {}", ticker)));
                }
            }
            debug!("Ticker validation passed: {:?}", ticker_list);
            let ticker_values = ticker_list
                .iter()
                .map(|t| format!("'{}'", t.replace('\'', "''")))
                .collect::<Vec<_>>()
                .join(", ");
            format!("AND ticker IN ({})", ticker_values)
        }
        _ => {
            debug!("No ticker filter applied, querying all tickers");
            String::new()
        }
    };

    let query_sql = format!(
        "
        SELECT
            ticker,
            timestamp,
            open_price,
            high_price,
            low_price,
            close_price,
            volume,
            volume_weighted_average_price,
            transactions
        FROM read_parquet('{}', hive_partitioning=1)
        WHERE (year::int * 10000 + month::int * 100 + day::int) BETWEEN {} AND {}
        {}
        ORDER BY timestamp, ticker
        ",
        s3_glob, start_date_int, end_date_int, ticker_filter
    );

    debug!("Executing query SQL: {}", query_sql);

    info!("Preparing DuckDB statement");
    let mut statement = connection.prepare(&query_sql)?;

    info!("Executing query and mapping results");
    let equity_bars: Vec<EquityBar> = statement
        .query_map([], |row| {
            Ok(EquityBar {
                ticker: row.get(0)?,
                timestamp: row.get(1)?,
                open_price: row.get(2)?,
                high_price: row.get(3)?,
                low_price: row.get(4)?,
                close_price: row.get(5)?,
                volume: row.get(6)?,
                volume_weighted_average_price: row.get(7)?,
                transactions: row.get(8)?,
            })
        })?
        .collect::<Result<Vec<_>, _>>()
        .map_err(|e| {
            warn!("Failed to map query results: {}", e);
            Error::Other(format!("Failed to map query results: {}", e))
        })?;

    info!("Query returned {} equity bar records", equity_bars.len());

    if equity_bars.is_empty() {
        warn!(
            "No equity bar data found for date range {} to {}",
            start_timestamp, end_timestamp
        );
    }

    debug!("Creating DataFrame from equity bars");
    let equity_bars_dataframe = create_equity_bar_dataframe(equity_bars);

    let mut buffer = Vec::new();
    {
        let cursor = Cursor::new(&mut buffer);
        let writer = ParquetWriter::new(cursor);
        writer
            .finish(&mut equity_bars_dataframe?.clone())
            .map_err(|e| Error::Other(format!("Failed to write parquet: {}", e)))?;
    }

    info!("Query returned {} bytes of parquet data", buffer.len());

    Ok(buffer)
}

#[derive(Deserialize)]
pub struct PredictionQuery {
    pub ticker: String,
    pub timestamp: DateTime<Utc>,
}

pub async fn query_predictions_dataframe_from_s3(
    state: &State,
    predictions_query: Vec<PredictionQuery>,
) -> Result<DataFrame, Error> {
    info!(
        "Querying predictions for {} ticker/timestamp pairs",
        predictions_query.len()
    );
    let connection = create_duckdb_connection().await?;

    let mut s3_paths = Vec::new();
    let mut tickers = Vec::new();

    for prediction_query in predictions_query.iter() {
        let year = prediction_query.timestamp.format("%Y");
        let month = prediction_query.timestamp.format("%m");
        let day = prediction_query.timestamp.format("%d");

        let s3_path = format!(
            "s3://{}/equity/predictions/daily/year={}/month={}/day={}/data.parquet",
            state.bucket_name, year, month, day
        );

        debug!(
            "Adding S3 path for ticker {} at {}/{}/{}: {}",
            prediction_query.ticker, year, month, day, s3_path
        );

        s3_paths.push(s3_path);

        tickers.push(prediction_query.ticker.clone());
    }

    if s3_paths.is_empty() {
        warn!("No prediction query positions provided");
        return Err(Error::Other("No positions provided".into()));
    }

    info!("Querying {} S3 files for tickers: {:?}", s3_paths.len(), tickers);

    let s3_paths_query = s3_paths
        .iter()
        .map(|path| format!("SELECT * FROM '{}'", path))
        .collect::<Vec<_>>()
        .join(" UNION ALL ");

    let tickers_query = tickers
        .iter()
        .map(|ticker| format!("'{}'", ticker))
        .collect::<Vec<_>>()
        .join(", ");

    let query = format!(
        "
        SELECT
            ticker,
            timestamp,
            quantile_10,
            quantile_50,
            quantile_90
        FROM ({})
        WHERE ticker IN ({})
        ORDER BY timestamp, ticker
        ",
        s3_paths_query, tickers_query,
    );

    debug!("Executing export SQL: {}", query);

    info!("Preparing predictions query statement");
    let mut statement = connection.prepare(&query)?;

    info!("Executing predictions query and mapping results");
    let predictions: Vec<Prediction> = statement
        .query_map([], |row| {
            Ok(Prediction {
                ticker: row.get(0)?,
                timestamp: row.get(1)?,
                quantile_10: row.get(2)?,
                quantile_50: row.get(3)?,
                quantile_90: row.get(4)?,
            })
        })?
        .collect::<Result<Vec<_>, _>>()
        .map_err(|e| {
            warn!("Failed to map predictions query results: {}", e);
            Error::Other(format!("Failed to map query results: {}", e))
        })?;

    info!("Query returned {} prediction records", predictions.len());

    debug!("Creating predictions DataFrame");
    let predictions_dataframe = create_predictions_dataframe(predictions)?;

    info!(
        "Predictions DataFrame created with {} rows",
        predictions_dataframe.height()
    );

    Ok(predictions_dataframe)
}

pub async fn query_portfolio_dataframe_from_s3(
    state: &State,
    timestamp: Option<DateTime<Utc>>,
) -> Result<DataFrame, Error> {
    info!(
        "Querying portfolio data, timestamp filter: {:?}",
        timestamp.map(|ts| ts.to_string())
    );
    let connection = create_duckdb_connection().await?;

    let query = match timestamp {
        Some(ts) => {
            let year = ts.format("%Y");
            let month = ts.format("%m");
            let day = ts.format("%d");
            let s3_path = format!(
                "s3://{}/equity/portfolios/daily/year={}/month={}/day={}/data.parquet",
                state.bucket_name, year, month, day
            );
            info!(
                "Querying specific date portfolio: {}/{}/{}",
                year, month, day
            );

            format!(
                "
                SELECT
                    ticker,
                    timestamp,
                    side,
                    dollar_amount,
                    action
                FROM '{}'
                ORDER BY timestamp, ticker
                ",
                s3_path
            )
        }
        None => {
            let s3_wildcard = format!(
                "s3://{}/equity/portfolios/daily/**/*.parquet",
                state.bucket_name
            );
            info!(
                "Querying most recent portfolio using hive partitioning: {}",
                s3_wildcard
            );

            format!(
                "
                WITH partitioned_data AS (
                    SELECT
                        ticker,
                        timestamp,
                        side,
                        dollar_amount,
                        year,
                        month,
                        day
                    FROM read_parquet('{}', hive_partitioning=1)
                ),
                max_date AS (
                    SELECT MAX(year::int * 10000 + month::int * 100 + day::int) as date_int
                    FROM partitioned_data
                )
                SELECT
                    ticker,
                    timestamp,
                    side,
                    dollar_amount,
                    action
                FROM partitioned_data
                WHERE (year::int * 10000 + month::int * 100 + day::int) = (SELECT date_int FROM max_date)
                ORDER BY timestamp, ticker
                ",
                s3_wildcard
            )
        }
    };

    debug!("Executing query SQL: {}", query);

    info!("Preparing portfolio query statement");
    let mut statement = connection.prepare(&query)?;

    info!("Executing portfolio query and mapping results");
    let portfolios: Vec<Portfolio> = statement
        .query_map([], |row| {
            Ok(Portfolio {
                ticker: row.get::<_, String>(0)?,
                timestamp: row.get::<_, i64>(1)?,
                side: row.get::<_, String>(2)?,
                dollar_amount: row.get::<_, f64>(3)?,
                action: row.get::<_, String>(4)?,
            })
        })?
        .collect::<Result<Vec<_>, _>>()
        .map_err(|e| {
            warn!("Failed to map portfolio query results: {}", e);
            Error::Other(format!("Failed to map query results: {}", e))
        })?;

    info!("Query returned {} portfolio records", portfolios.len());

    debug!("Creating portfolio DataFrame");
    let portfolio_dataframe = create_portfolio_dataframe(portfolios)?;

    info!(
        "Portfolio DataFrame created with {} rows",
        portfolio_dataframe.height()
    );

    Ok(portfolio_dataframe)
}

pub async fn read_equity_details_dataframe_from_s3(state: &State) -> Result<DataFrame, Error> {
    info!("Reading equity details CSV from S3");

    let key = "equity/details/categories.csv";

    let response = state
        .s3_client
        .get_object()
        .bucket(&state.bucket_name)
        .key(key)
        .send()
        .await
        .map_err(|e| Error::Other(format!("Failed to get object from S3: {}", e)))?;

    let bytes = response
        .body
        .collect()
        .await
        .map_err(|e| Error::Other(format!("Failed to read response body: {}", e)))?
        .into_bytes();

    let csv_content = String::from_utf8(bytes.to_vec())
        .map_err(|e| Error::Other(format!("Failed to convert bytes to UTF-8: {}", e)))?;

    info!(
        "Successfully read CSV from S3, size: {} bytes",
        csv_content.len()
    );

    let dataframe = create_equity_details_dataframe(csv_content)?;

    info!(
        "Successfully processed DataFrame with {} rows",
        dataframe.height()
    );

    Ok(dataframe)
}
